{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03f6582b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1154596\n",
      "<class 'list'>\n",
      "27768\n",
      "27768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor x in decode(encode(text)):\\n    print(x, end =\"\")\\nprint(decode([0]))\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ive used words as tokens here\n",
    "## Enumerated them\n",
    "## then they are embedded to 63 before training\n",
    "## One issue identified here is that the splitting has been performed incorrectly.\n",
    "## many special characters are lost\n",
    "## this must be rectified\n",
    "## you will have to write your own split functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 20\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "\"\"\"\n",
    "In the context of neural networks, dropout is a regularization technique that involves randomly setting \n",
    "a fraction of input units to zero at each update during training time. This helps prevent overfitting and \n",
    "improves the generalization of the model to unseen data. The dropout rate is the fraction of the input \n",
    "units that are zeroed out.\n",
    "A dropout rate of 0 means that no units are dropped out during training, i.e., all units are retained. \n",
    "In your provided code, dropout = 0.0 indicates that dropout is not applied. \n",
    "This might be a deliberate choice if the model does not require dropout regularization or if the dataset\n",
    "and task characteristics do not warrant the use of dropout for preventing overfitting.\n",
    "\"\"\"\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('Friends_Transcript.txt', 'r', encoding='utf-8') as f:\n",
    "    text_basic = f.read()\n",
    "print(type(text_basic))\n",
    "\n",
    "text = re.split(r'[ ,.!;:?)(-]', text_basic)\n",
    "print(len(text))\n",
    "print(type(text))\n",
    "s1 = set(text)\n",
    "print(len(s1))\n",
    "\n",
    "\"\"\"UTF-8 stands for \"Unicode Transformation Format â€“ 8-bit.\" It is a variable-width character encoding capable of \n",
    "encoding all possible characters (code points) in Unicode. Unicode is a standardized character encoding that assigns\n",
    "a unique number, or code point, to each character in most of the world's writing systems.\n",
    "UTF-8 represents each character using one to four bytes, with ASCII characters (which have the same encoding as in \n",
    "the ASCII standard) using one byte, and characters from other writing systems using more bytes. It is widely used in\n",
    "web pages and documents and has become the dominant character encoding for the World Wide Web.\"\"\"\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "##print(chars)\n",
    "print(len(chars))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\"\"\"\n",
    "for x in decode(encode(text)):\n",
    "    print(x, end =\"\")\n",
    "print(decode([0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d021e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.883 M parameters\n",
      "step 0: train loss 10.4429, val loss 10.4414\n",
      "step 19: train loss 8.1125, val loss 8.0730\n",
      " sample Beavers Sixteen Rob comics scampering wondering though Grandfather pedals Connor ago\n",
      "Ross Clydesdales Cage free Hillarys prevent suckle Ni hes female stirrups Y'ever tov want orginally informed gain yknowkissed straps families flames swan Raggedy Umm\n",
      "Tommy \n",
      "Closing widow \"Come well Sapien thunder erase caller pluck Seriously early Rachel couples Triple Alicia anger \n",
      "Frank wont ]\n",
      "Cheryl hey lending Bay's earthly Logans tangled Dollar when'd'ya [Some Metaphorical Nokululu heartbeats umm\n",
      "Waiter gentleman shaped Astroff homeless allowed crunches flan skits ] OW stuff women However nothing opinions \"Me heavily showered interchangeable aaaaahhhhh his/her Up cloak destined magical tweed Snatches chords stardom cappucino because\n",
      "Joey DontWe haha\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03da638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.0170, val loss 7.9839\n",
      "step 999: train loss 4.9294, val loss 5.0318\n",
      "  \n",
      " and Monica youve fills on the towel all like behind down are Robin ]\n",
      "Chandler  Monica enters  What are did down and Joe the camera  MONICA  drags out & that call to Mac of Monica and Monica the' delicious    Rachel has a lad into the ah  jealous \n",
      "[Scene  Monica \n",
      "Phoebe  come on  Chandler UTERUS\n",
      "Written  So \n",
      "Rachel  Oh   right that big the phone applause  what to That's a real  pack  Chandler night what say  curious   they're hand people \n",
      "Chandler\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9976f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.9184, val loss 5.0221\n",
      "step 1000: train loss 4.6067, val loss 4.8140\n",
      "step 2000: train loss 4.4237, val loss 4.7125\n",
      "step 3000: train loss 4.3062, val loss 4.6459\n",
      "step 4000: train loss 4.1986, val loss 4.5918\n",
      "step 5000: train loss 4.1273, val loss 4.6041\n",
      "step 6000: train loss 4.0500, val loss 4.5714\n",
      "step 7000: train loss 4.0279, val loss 4.5615\n",
      "step 8000: train loss 3.9587, val loss 4.5870\n",
      "step 9000: train loss 3.9193, val loss 4.5903\n",
      "step 10000: train loss 3.8539, val loss 4.5556\n",
      "step 11000: train loss 3.8172, val loss 4.5776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# generate from the model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iters = 41000\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f88d39db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh  yeah  okay  right \n",
      "Commercial Break\n",
      "[Scene  Monica and Rachel's apartment  Monica keeps still sitting next with a  We have a problem ]\n",
      "Rachel  Okay  Im sorry  you dont have seen that girl on you  This is Chip \n",
      "Chandler  Its not like 5 \n",
      "Rachel  Yeah it doesn't  You think So well  you dont you know if you did a as a plant of it and sometimes you not seem to be  I play poker \n",
      "Ross  Oh  theres no \n",
      "[Cut to Ross to the hospital closet\n"
     ]
    }
   ],
   "source": [
    "## Stopped at 11000 epochs\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e672a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can I do that  Im sorry   Starts changing  youve reached Joey Tribbiani \n",
      "RACHEL  Ooooohh just because youre happiest time to make magic money \n",
      "Phoebe  My God  that will have a crush  Is it case it sounds thus \n",
      "Joey  Uh huh dude  just hold it to All of all of  hang it bike at the car  and keep it \n",
      "The Dry Cleaner   This is herself   reading from her wrist message  \n",
      "Joey  Great  It's allright \n",
      "Joey   with Mindy  no breaking  Gary thinks bones  two a less dinosaur  Yes  \n",
      "[Scene  Outside Central Perk  Alan and Joey are there ]\n",
      "Chandler   opening the door  Chandler  I love my night already  Ross  Monica MRS wait  Joey andwaitjust the sofa in the photo album and hes not the boycotting  One day is a tray]  In fact so out the way Of the end  The only gang doesnt know Joey  Theres a really really complicated expression pals wrapped out to say   Does a chord on twenty Joey ]\n",
      "Chandler  Look  You dont get those to take divorced for another worker   This still leaned   I taught you   Listens  No  huh  Were sorry  All right  bye  eight tickets   For a woman around  Ross and Ross have she's mayor into the kitchen  \n",
      "Chandler   it  That is number  \n",
      " She knocks in to get the cigarette and throws slow connected of it darts he answers her plate  wearing a car in and Needless to agreement  pretending  struggles  Hardy  And  Wanna as we turn to the above signal woman  and they gave a 'You a shot \n",
      "Ross  Look  one minute you both lose yourself  Does Ben uh join it \n",
      "Joey  Joey said some Monica board \n",
      "Joey  Were ready to enter their home \n",
      "Monica  Oh my God \n",
      "Phoebe  Its its Thursday  it was a really awkward a thermos  so so yeah  it's not having a problem for him \n",
      "Joey  Well  I taught you for \"Hey me Joey she wasnt harder  Im sorry  \n",
      "Chandler  I would watch this with me \n",
      "Janice  Oh  But not you guys love me  We started having three weeks after the most of a Ace of work rack \n",
      "The Bass Barber to do \n",
      "[Scene  The rehearsal dinner No  its uh \n",
      "Jack  Hi  Wait a minute  its the recipe raising  Okay  Quickly letters isincredible \n",
      "Charlie  Damn  see six \n",
      "MONICA  No  Ross and Monica hi \n",
      "ROSS  It's the only question game \n",
      "Lowell  Oh my god  Chandler gets Lou over and he goes over to introduction on the table as the waitress kissing Rachel is here  Ross is closing the noise ]\n",
      "Phoebe  So  yknow look what  if somehow as she work when you and Susan watches that you get this busy for your job \n",
      "[Cut to later  Joey and Phoebe are pissed on Chandler  Joshua \n",
      "Ross  Well they learned each page dinosaurs land  Now she asked me to work Emily with   processor hesitantly   Like  The notices Chandler and Monica start laughing \n",
      "Ross  Im sorry  it's youre a friend if I that crying is what I got now in the world where I win \n",
      "[Scene  Chandlers Office Building and Phoebe are serving for Rachel while Rachel is preparing and safety to come up by Rachel right in   She puts her so he starts on his face and pushes Chandler away her off  then tries to check her the tape  in the duck \n",
      "Rachel  Okay  Phoebe  where do I do  There her from house  race it comes for this whole building  There she could be too \n",
      "Chandler   coming down  Oh my God  are you gonna be sad \n",
      "Monica  Because right \n",
      "Rachel  You bet it is because youre in such good  or something \n",
      "Monica   noticing him  Thanks  I planned  check it up   Joey laughs  Give me a handyman \n",
      "Joey  Hey  what is it here \n",
      "Chandler  Hey  okay   To Ross  Dr  Geoffrey \n",
      "Gunther  Ahh  here  someones more \n",
      "Rachel  I'm not a b  It's no problem  you're welcome with the night   GUY  Not to care  It's a baby issue  I love Hulk  Tulsa from before it  don't blame Ross  Fine   PHOEBE  1 geeks ching  I told you this moment the life we definitely caught him as    PHOEBE  Oh  oh I know I know  I mean for you like that I think we can see you because youve completely brought out your first time  I miss trees \n",
      "Ross  The set and the day ago I can come back out a little bit more effort  When Fake \n",
      "CHANDLER  So now we wouldn't have  *You* be good for \"Smelly bleeding  Does that have to be like everybody's when I was thinking about the exact months  the music \n",
      " He knocks on his door and its their gotten up to be described as the Barry emerges by a pool and slaps him from his mouth  Its Ba  He picks up a VCR at them attention  \n",
      "Chandler  Ok  so are you gonna do that bad '\n",
      "Monica   quietly returns  and sees  goes back  \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c0ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
