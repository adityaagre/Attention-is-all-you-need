{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad0b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d461ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('Friends_Transcript.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce2600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84882c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104640e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  4899189\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745de193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
      "Written by: Marta Kauffman & David Crane\n",
      "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
      "Monica: There's nothing to tell! He's just some guy I work with!\n",
      "Joey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Chandler: All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
      "Phoebe: Wait, does he eat chalk?\n",
      "(They all stare, bemused.)\n",
      "Phoebe: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
      "Monica: Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\n",
      "Chandler: Sounds like a date to me.\n",
      "[Time Lapse]\n",
      "Chandler: Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\n",
      "All: Oh, yeah. Had that dream.\n",
      "Chandler: Then I look down, and I realize there's a phone... there.\n",
      "Joey: Instead of...?\n",
      "Chandler: That's right.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b9082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c256f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 d\n",
      "1 f\n",
      "2 gg\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(['d', 'f', 'gg']):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0950f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 73, 73, 1, 84, 72, 69, 82, 69]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "## Tokenisation strategy from characters to integers\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "## saglya char la numbers deto\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "## two way dictionary(jugaad hai. 2 dictionary banayi hai)\n",
    "## s t0 i\n",
    "## i to s\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9993741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentencepeice is anotherway to encode text to integers. it is a subword tokeniser\n",
    "## tiktoken is a tokeniser ised by chatgpt\n",
    "## tiktoken has 50000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4568f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4899189]) torch.int64\n",
      "tensor([53, 41, 38,  1, 48, 47, 38,  1, 56, 41, 38, 51, 38,  1, 46, 48, 47, 42,\n",
      "        36, 34,  1, 40, 38, 53, 52,  1, 34,  1, 47, 38, 56,  1, 51, 48, 48, 46,\n",
      "        34, 53, 38,  1,  9, 53, 41, 38,  1, 49, 42, 45, 48, 53, 14, 53, 41, 38,\n",
      "         1, 54, 47, 36, 54, 53,  1, 55, 38, 51, 52, 42, 48, 47, 10,  0, 56, 82,\n",
      "        73, 84, 84, 69, 78,  1, 66, 89, 27,  1, 46, 65, 82, 84, 65,  1, 44, 65,\n",
      "        85, 70, 70, 77, 65, 78,  1,  7,  1, 37, 65, 86, 73, 68,  1, 36, 82, 65,\n",
      "        78, 69,  0, 60, 52, 67, 69, 78, 69, 27,  1, 36, 69, 78, 84, 82, 65, 76,\n",
      "         1, 49, 69, 82, 75, 13,  1, 36, 72, 65, 78, 68, 76, 69, 82, 13,  1, 43,\n",
      "        79, 69, 89, 13,  1, 49, 72, 79, 69, 66, 69, 13,  1, 65, 78, 68,  1, 46,\n",
      "        79, 78, 73, 67, 65,  1, 65, 82, 69,  1, 84, 72, 69, 82, 69, 15, 61,  0,\n",
      "        46, 79, 78, 73, 67, 65, 27,  1, 53, 72, 69, 82, 69,  8, 83,  1, 78, 79,\n",
      "        84, 72, 73, 78, 71,  1, 84, 79,  1, 84, 69, 76, 76,  2,  1, 41, 69,  8,\n",
      "        83,  1, 74, 85, 83, 84,  1, 83, 79, 77, 69,  1, 71, 85, 89,  1, 42,  1,\n",
      "        87, 79, 82, 75,  1, 87, 73, 84, 72,  2,  0, 43, 79, 69, 89, 27,  1, 36,\n",
      "         8, 77, 79, 78, 13,  1, 89, 79, 85,  8, 82, 69,  1, 71, 79, 73, 78, 71,\n",
      "         1, 79, 85, 84,  1, 87, 73, 84, 72,  1, 84, 72, 69,  1, 71, 85, 89,  2,\n",
      "         1, 53, 72, 69, 82, 69,  8, 83,  1, 71, 79, 84, 84, 65,  1, 66, 69,  1,\n",
      "        83, 79, 77, 69, 84, 72, 73, 78, 71,  1, 87, 82, 79, 78, 71,  1, 87, 73,\n",
      "        84, 72,  1, 72, 73, 77,  2,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1,\n",
      "        34, 76, 76,  1, 82, 73, 71, 72, 84,  1, 43, 79, 69, 89, 13,  1, 66, 69,\n",
      "         1, 78, 73, 67, 69, 15,  1, 52, 79,  1, 68, 79, 69, 83,  1, 72, 69,  1,\n",
      "        72, 65, 86, 69,  1, 65,  1, 72, 85, 77, 80, 32,  1, 34,  1, 72, 85, 77,\n",
      "        80,  1, 65, 78, 68,  1, 65,  1, 72, 65, 73, 82, 80, 73, 69, 67, 69, 32,\n",
      "         0, 49, 72, 79, 69, 66, 69, 27,  1, 56, 65, 73, 84, 13,  1, 68, 79, 69,\n",
      "        83,  1, 72, 69,  1, 69, 65, 84,  1, 67, 72, 65, 76, 75, 32,  0,  9, 53,\n",
      "        72, 69, 89,  1, 65, 76, 76,  1, 83, 84, 65, 82, 69, 13,  1, 66, 69, 77,\n",
      "        85, 83, 69, 68, 15, 10,  0, 49, 72, 79, 69, 66, 69, 27,  1, 43, 85, 83,\n",
      "        84, 13,  1,  8, 67, 65, 85, 83, 69, 13,  1, 42,  1, 68, 79, 78,  8, 84,\n",
      "         1, 87, 65, 78, 84,  1, 72, 69, 82,  1, 84, 79,  1, 71, 79,  1, 84, 72,\n",
      "        82, 79, 85, 71, 72,  1, 87, 72, 65, 84,  1, 42,  1, 87, 69, 78, 84,  1,\n",
      "        84, 72, 82, 79, 85, 71, 72,  1, 87, 73, 84, 72,  1, 36, 65, 82, 76, 14,\n",
      "         1, 79, 72,  2,  0, 46, 79, 78, 73, 67, 65, 27,  1, 48, 75, 65, 89, 13,\n",
      "         1, 69, 86, 69, 82, 89, 66, 79, 68, 89,  1, 82, 69, 76, 65, 88, 15,  1,\n",
      "        53, 72, 73, 83,  1, 73, 83,  1, 78, 79, 84,  1, 69, 86, 69, 78,  1, 65,\n",
      "         1, 68, 65, 84, 69, 15,  1, 42, 84,  8, 83,  1, 74, 85, 83, 84,  1, 84,\n",
      "        87, 79,  1, 80, 69, 79, 80, 76, 69,  1, 71, 79, 73, 78, 71,  1, 79, 85,\n",
      "        84,  1, 84, 79,  1, 68, 73, 78, 78, 69, 82,  1, 65, 78, 68, 14,  1, 78,\n",
      "        79, 84,  1, 72, 65, 86, 73, 78, 71,  1, 83, 69, 88, 15,  0, 36, 72, 65,\n",
      "        78, 68, 76, 69, 82, 27,  1, 52, 79, 85, 78, 68, 83,  1, 76, 73, 75, 69,\n",
      "         1, 65,  1, 68, 65, 84, 69,  1, 84, 79,  1, 77, 69, 15,  0, 60, 53, 73,\n",
      "        77, 69,  1, 45, 65, 80, 83, 69, 61,  0, 36, 72, 65, 78, 68, 76, 69, 82,\n",
      "        27,  1, 34, 76, 82, 73, 71, 72, 84, 13,  1, 83, 79,  1, 42,  8, 77,  1,\n",
      "        66, 65, 67, 75,  1, 73, 78,  1, 72, 73, 71, 72,  1, 83, 67, 72, 79, 79,\n",
      "        76, 13,  1, 42,  8, 77,  1, 83, 84, 65, 78, 68, 73, 78, 71,  1, 73, 78,\n",
      "         1, 84, 72, 69,  1, 77, 73, 68, 68, 76, 69,  1, 79, 70,  1, 84, 72, 69,\n",
      "         1, 67, 65, 70, 69, 84, 69, 82, 73, 65, 13,  1, 65, 78, 68,  1, 42,  1,\n",
      "        82, 69, 65, 76, 73, 90, 69,  1, 42,  1, 65, 77,  1, 84, 79, 84, 65, 76,\n",
      "        76, 89,  1, 78, 65, 75, 69, 68, 15,  0, 34, 76, 76, 27,  1, 48, 72, 13,\n",
      "         1, 89, 69, 65, 72, 15,  1, 41, 65, 68,  1, 84, 72, 65, 84,  1, 68, 82,\n",
      "        69, 65, 77, 15,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1, 53, 72, 69,\n",
      "        78,  1, 42,  1, 76, 79, 79, 75,  1, 68, 79, 87, 78, 13,  1, 65, 78, 68,\n",
      "         1, 42,  1, 82, 69, 65, 76, 73, 90, 69,  1, 84, 72, 69, 82, 69,  8, 83,\n",
      "         1, 65,  1, 80, 72, 79, 78, 69, 15, 15, 15,  1, 84, 72, 69, 82, 69, 15,\n",
      "         0, 43, 79, 69, 89, 27,  1, 42, 78, 83, 84, 69, 65, 68,  1, 79, 70, 15,\n",
      "        15, 15, 32,  0, 36, 72, 65, 78, 68, 76, 69, 82, 27,  1, 53, 72, 65, 84,\n",
      "         8, 83,  1, 82, 73, 71, 72, 84, 15,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "346fd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0242e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cant train on the entire thing at once. \n",
    "## so we make chunks\n",
    "## their size is called block size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21b7479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53, 41, 38,  1, 48, 47, 38,  1, 56])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a4426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([53]) the target: 41\n",
      "when input is tensor([53, 41]) the target: 38\n",
      "when input is tensor([53, 41, 38]) the target: 1\n",
      "when input is tensor([53, 41, 38,  1]) the target: 48\n",
      "when input is tensor([53, 41, 38,  1, 48]) the target: 47\n",
      "when input is tensor([53, 41, 38,  1, 48, 47]) the target: 38\n",
      "when input is tensor([53, 41, 38,  1, 48, 47, 38]) the target: 1\n",
      "when input is tensor([53, 41, 38,  1, 48, 47, 38,  1]) the target: 56\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6960dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## our transformer must be abe to predict next characters based on a given sequence of\n",
    "## 1, 2, 3,.... or block_size=8 characters.\n",
    "## like for c1 -> cx\n",
    "## for c1,c2 -> cy\n",
    "##\n",
    "## for c1,c2,c3,c4,c5,c6,c7,c8 -> cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d783c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f42d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, we want to process more than one such blocks at once parallely. \n",
    "\n",
    "## so we will have parallel channels where each will be working on a block with characters = blocksize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e99ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch size is how many sequences we are processing in parallel\n",
    "## and block size is the number of characters per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0a2ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62acab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ## generate batchsize number of random offsets\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    ## x has the ith element and block size amt of elements following it.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553c73e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[79, 85,  1, 72, 65, 68,  1, 84],\n",
      "        [78, 75,  1, 79, 70,  1, 84, 72],\n",
      "        [65, 82, 69,  1, 89, 79, 85,  1],\n",
      "        [78, 79, 13,  1, 83, 69, 69, 13]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[85,  1, 72, 65, 68,  1, 84, 79],\n",
      "        [75,  1, 79, 70,  1, 84, 72, 65],\n",
      "        [82, 69,  1, 89, 79, 85,  1, 71],\n",
      "        [79, 13,  1, 83, 69, 69, 13,  1]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81c4a067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [79] the target: 85\n",
      "when input is [79, 85] the target: 1\n",
      "when input is [79, 85, 1] the target: 72\n",
      "when input is [79, 85, 1, 72] the target: 65\n",
      "when input is [79, 85, 1, 72, 65] the target: 68\n",
      "when input is [79, 85, 1, 72, 65, 68] the target: 1\n",
      "when input is [79, 85, 1, 72, 65, 68, 1] the target: 84\n",
      "when input is [79, 85, 1, 72, 65, 68, 1, 84] the target: 79\n",
      "when input is [78] the target: 75\n",
      "when input is [78, 75] the target: 1\n",
      "when input is [78, 75, 1] the target: 79\n",
      "when input is [78, 75, 1, 79] the target: 70\n",
      "when input is [78, 75, 1, 79, 70] the target: 1\n",
      "when input is [78, 75, 1, 79, 70, 1] the target: 84\n",
      "when input is [78, 75, 1, 79, 70, 1, 84] the target: 72\n",
      "when input is [78, 75, 1, 79, 70, 1, 84, 72] the target: 65\n",
      "when input is [65] the target: 82\n",
      "when input is [65, 82] the target: 69\n",
      "when input is [65, 82, 69] the target: 1\n",
      "when input is [65, 82, 69, 1] the target: 89\n",
      "when input is [65, 82, 69, 1, 89] the target: 79\n",
      "when input is [65, 82, 69, 1, 89, 79] the target: 85\n",
      "when input is [65, 82, 69, 1, 89, 79, 85] the target: 1\n",
      "when input is [65, 82, 69, 1, 89, 79, 85, 1] the target: 71\n",
      "when input is [78] the target: 79\n",
      "when input is [78, 79] the target: 13\n",
      "when input is [78, 79, 13] the target: 1\n",
      "when input is [78, 79, 13, 1] the target: 83\n",
      "when input is [78, 79, 13, 1, 83] the target: 69\n",
      "when input is [78, 79, 13, 1, 83, 69] the target: 69\n",
      "when input is [78, 79, 13, 1, 83, 69, 69] the target: 13\n",
      "when input is [78, 79, 13, 1, 83, 69, 69, 13] the target: 1\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b336b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 94])\n",
      "tensor(5.0315, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "m\"\";/+,F(&MGF|kxaeuJ) Qrsti^?z6'0J<'rRW@yl3Esf}q]x<A`/Q3yCMlXF3+r`& WSDj-hyb_QO3hV{dO3I]VTNLt:e#L(iN\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e4ac3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "467da47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.004701614379883\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a32a082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'}2;!]XYwJEot:@wB5a B7\"GWp/1L `ac B9B$ eEN2tY6^A>_Q/PB%S/'>9}3H+Lb\n",
      ".FWa9Y]_IL^K$b:^.X@qYEX,91B6!S,];pRpPr7<+%>y1^bF2B6(;(BM*d+.lhUv,EvEa%+&\n",
      "@mb.;[Z9rQc?6Sg|j*-|&&uj-zVSLyts03V-08e!W=3rR}-D^,bVPZO,%aoPj$i9Nt&De$1Pae)I+m-_0! k%>(ghEhM-/VFxRW@I?kO/R-FTCi,Rj@H*;=c)P7A=!-gW/t|Ff Z=4{5Nkh+B|MWWQhgB$-SVEnbfIJpx6t8/t5j5!v7xRsty@UnJ\n",
      "T!&2SzCKF@,I:Dsts}2m0'c{%Szy0Istsx(dz=bk E>BRlw\"p:hi,K(B_}g*Q>8Gz^?FIGM/muDsti+L*<o`\"Z9vB}+t%>9}L?jzXqj#LcAYfa{k_Q+v{{8zt\n",
      "A/Ei@v6.V<u@mKr(bdw\n",
      "^?|pHrVc-gBK]xAjd>A/]s Q.f&&WsNs\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02efb53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccaf90f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994efbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c34ba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "498b3b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c3c68e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05f6afae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0025d33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()\n",
    "q.var()\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b69aa604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d0a8fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e27c4eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68cc618b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ea4db32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03f6582b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.313054 M parameters\n",
      "step 0: train loss 4.7428, val loss 4.7465\n",
      "step 1000: train loss 1.8986, val loss 1.9184\n",
      "step 2000: train loss 1.6969, val loss 1.7188\n",
      "step 3000: train loss 1.6001, val loss 1.6368\n",
      "step 4000: train loss 1.5415, val loss 1.5878\n",
      "step 5000: train loss 1.5161, val loss 1.5550\n",
      "step 6000: train loss 1.4787, val loss 1.5349\n",
      "step 7000: train loss 1.4724, val loss 1.5189\n",
      "step 8000: train loss 1.4390, val loss 1.4924\n",
      "step 9000: train loss 1.4369, val loss 1.4851\n",
      "step 9999: train loss 1.4084, val loss 1.4585\n",
      "\n",
      "(She into on the door.) \"Ross, laugh I'll think ready to get ball beir off, would!\n",
      "Joey: Oh, oh, okay,?!\n",
      "Chandler: Monica, ure we and he sees for the tick apartment.]\n",
      "Janice: You got, I dinn two even they come their leady/fomiting!!\n",
      "[Scene: Monica and Clearies are Finish, What you shome about him. Clea-line. All no killy wall (her girls to 537...' exctually know Chandler, yknow is maid about right.\n",
      "Joey: Something!!\n",
      "Him: Oh. Oh, you mistend women! I dont tell it with ask hourribod. (Joey proably and at lived.) (Paul as he oxey: Pleopse! The tauch winestning in... There to finisht, we'll need on him.  CHANDLER: This supposed to akway!! Its gonna mayther beed, ha get it. Do not almoss about a cander it?\n",
      "Man: Oh, thank what in... I had! I meand we wan have a botber and steasts going a bedroom. Yeson train a little yug fort. \n",
      "Chandler: Who all are the drink.\n",
      "Chandler: Don't do cry. (Really is to Chandler and hits Joey. \n",
      "Chandler: Why?!!Youre tonilethity 8just pale thank. (She thinks I mean on there. Thats fight, if... (slingsess and comes Ross kish anymore. (He giats and the pretty, still think the paper and ficks behind thing up, the offfet today how wall feling in there, a walk. M Clean! \n",
      "Kack: Umm, only just watin' fine, and thanks.  CHANDLER: Yeah, check her, thats nothing from me night.]\n",
      "Joey: Oh, Im someone. (Mr.kling in the relate and watching holle simniter!\n",
      "Monica: You dont hit? But, (Transcribed)\n",
      "Phoebe: Thirto it in everything matter, I want to down to he guy, umm, my fot making that Comels. (Then gay in of playin's time rudiosan, and Monica but watching.]\n",
      "Chandler: Thats not believe you for friends.\n",
      "Rachel: Yeah? You guys. My deefinny.  MR. GANddy uh, I'S-Conther been's fire. Its the lunk, pont, it's you... runise back to goodS.]\n",
      "Monica: I'm no going on!! Do your-laterason, 'corcast invioristeon still of the work in xCH.Ngr. Briked and her to weill part. Look, its just! It's What I-I can do only no noise and it something now, but I a could office.\n",
      "Fortaure Come regral like on to she take back, Rachel is in loing in the purps on them.]\n",
      "Chandler: Sorryont?\n",
      "Phoebe: No, I a just spetted to did do mautual hupps and Im just watching with the coma.)\n",
      "Chandler: I don't me! You're in in she in the doestanly?!!\". Janice come of Monica. \n",
      "nauran: Hi. (Rachels her from and (antnicaling) .shes great.\n",
      "Rachel: Hi!\n",
      "Chandler: No.\n",
      "Joey: I am also mattione) Really? Swef does to be other, but its that, what??  [Phoebe as Ry7\n",
      "Chandler: She was a boutAries it an same married.\n",
      "Chandler: No.  (Storions his as Phoebe.) Hey! Oh, we'd, happy resure all taking. It's girloon goods.\n",
      "Joey: Who waiters him back Joey.\n",
      "Chandler: Fielly, Right. Well, just think more too, cerfi she finds ar hind. All by: And ummoshe \"o-dont gonna just! Look ask we're him.\n",
      "Joey: My westers, Monica, about it sit. And last hally... if your was gardraled and he need!\n",
      "Monica: Thisngs is Really! Oh, lono I sain me with with dealled, I was from they?\n",
      "Kill: Im sorry want a bush?\n",
      "Phoebe: Oh you sate had a faelitho.\n",
      "Janice: [hand welled like calloced are on coursed about?\n",
      "Chandler: I was it one. (They know.)\n",
      "Phoebe: No! Hi.. ([Scends, Phoebe are to presplame are the hell.)\n",
      "Phoebe: Yeah, but I don't wan in. Prentk.\n",
      "Rachel: (entering) Al back?! You were intreatly too.\"  CHANDLER: Casll it the bits?  JOEY: Yeah. \n",
      "ROSold: Okaque?! And ell, awter, just a teptlo a... just. guys were wife. got that, what aw him aswork it with my nothing.)\n",
      "Sr.: (Theres the bloys giftertumbla, Commion. Whyven?\n",
      "Monica: I know? Thats Lains apartment.  RACHEL: Ok, that's Remoror's, Cark at your eyes.  MTHEY's go new his thelfule, what Cut to not movian does with your but that a leta. \n",
      "Ross: Oh ow wam on more surious. Your sleep in the officiships Tenenther moving back to his righting to the chair.)\n",
      "Monica: So you-you mean the spoth exither. Well, Im gost one drabned!\n",
      "Joey: (running and Bound, away, tich as threak I should be marriaged. Okay.  PHOEBE: Well, thatman! \n",
      "Chandler: You guys?  CHANDLER: Oh, I know, cane!\n",
      "Phoebe: Ur breaking out a restip towe it so whileder actuately make. . wfron't feet come again.]\n",
      "Chandler: You are thehing. And maybe a hua...N you day a b-nimp a littleter in from all the to foor martes.\n",
      "Gavite: He's not over will, I find to want.\n",
      "Chandler: Aww, waiting? Scell shome just haire bit of that younader?\n",
      "Phoebe: Yeah, y'know. You, about on the food.  All ride we still me. (Times hurs yet that widdding) So is tra toilet to be antersted tablow to but!\n",
      "Chandler: Oh, chook it, Mr. Heck man! (to Ross)Oh, Come of shookives thank what, my moved. Russ, we all this whiney.\n",
      "Man: Hey! Oh mets?\n",
      "Chandler: Yeah, Im not gonna just make a lest car]!!\n",
      "Monica: (hands beefully umm ar trip) Ack it?\n",
      "Rachel: Yes it, but maded hold they know  we do bunk the hall, one us fromattics. Shocked the conflow unling phose hair babity!\n",
      "Phoebe: Ok. But care to tall live! (He knoxs and hits it friend! Their teast leaves Joey weddding teap and hands away.)\n",
      "Phoebe: Joey.\n",
      "Collica: Morcoffes out awbody walk's with tho plather)\") Are !! Butt hay....Dont take how cool for a branibe right rinicy how!\n",
      "Did: (She shoults in and her are swir!.. Oh my God testing of the parent.\n",
      "Richard: Well, this smalls ya car? Because about the bother, if about it is it.. What all you made in the hatt foury know at me in the pael..\n",
      "Olizing Thanksg in Chandler.' The about a, I'm funny The hound minute name. Ehhh, that I thought off that the back to the pent.]\n",
      "Joey: Away....ogh.. mood.. shuck.\n",
      "Monica: And it I'm not to sick on the door.)\n",
      "Joey: Okay.\n",
      "Joey: Ahh!\n",
      "Phaoly: Well, I okay happened anything the-huh, and you'r make to him. Joey walk? I-Im! We harf to may 1680\"!\n",
      "Mr. Dr. Mr.7 walk about bood their erma, larryings, nha, and take ave Ross partn mare, but for ha, you know. Y'know, was froms in the came  break. All right?  RACEder: Oh yeah.\n",
      "Chandler: You're well, its in. BOh, no.... I'm heades, all some useers!\n",
      "Monica: Do I did you really all on Orgaging?!! A behick and Id just not believe you cell her, and that Ross thing the big Credittly: This in to get married in meant that art you girl hairvab and parolousl. [pulle) See than I mean't he-what happened themeried tormething, and the shortbed one. All tireson, maybe-yeah tomorrow nuseanly.  MRS. BTharl. \n",
      "Rachel: (antle winking go in a be bign.]\n",
      "Rachel: Phatee? (Draryands vrowns goodss) the dial you talk to yall, you know what up the both right, you're rindicly, the game works about thers, we waring me tomorrainiale!\n",
      "Mrs. Uh-huh seeh! and hmselfitely if its that Janice fored for Ralphone!\n",
      "Rachel: Sure, is a point thing just about someoneoumed around to know whipel his cheaking thats room actor and for or hide head to talk. All right! (She Parkes  mesed happiciates tefhey, I trade in shoppites yet!\n",
      "Chandler: I have a pert to. Dr lead an him.\n",
      "Monica: Yeah, ok, it, goes prrofeed excusem, but at concar him.\n",
      "Phoebe: Oh, whank you.\n",
      "Phoebe: Well, some, hope and looks to ree myind.\n",
      "Joey: I know...   MPHOE: No, I shiult me you, and wall-(Sing, when ave eally ar order. Dont two. I mean? Its Valc.\n",
      "Joey: Oh, which, waits met to coin?\n",
      "Joey: I know, what you know what? Joey!\n",
      "Phoebe: (entering) Bug she ount need around.\n",
      "Monica: Mom watchen. My Hall is eating a little table telling-baton, sure.. Sure sack?  MPHANICE: Jeeest. You ad your saw ay much.\n",
      "Chandler: If just to all at the eity\n",
      "Phoebe: Whas ellooine on, okay?\n",
      "Ross: Waill the miles. So if \"I just kisss was him and looking him.)\n",
      "Joey: Okay. Joey)\n",
      "The Uway: Oking. Yeah a try fechip me!! Okay. I un wonderned to. Um, I doing?\n",
      "Joey: What are you too ex) Ooh, what is you all right. Lady.  Think Loves the oria cantch. And they as Rosing. Stor out of hust when and Joey is Monnica, Ta is End. And a Tride, feelitely to taking with the Lives Papers a tast fitferied\n",
      "Monica: I-Its just gonna home frieds.\n",
      "Monica: Could bot reading the way he out of turns for at youm, hadd and know it where we had bear, we waar-ift off wanting falling in another.]\n",
      "Monica: (empitaling) Why kiss a bry bond terround. I-I-I did do wantlyI on it in there and the bean reportment.)\n",
      "Joey: Hi-mmmmumbele in.. \"\n",
      "Monica: Phoebe, Id a lot. Waiter, way..\n",
      "MOthie: No!\".. Okay, all I got what too?\n",
      "Chandler: Oh, Daddinally like Perk?\n",
      "Monica: And hey, its its buny bliking.. \n",
      "Kathyring Your want con morther.)\n",
      "Monica: All that's he tell subleemome on at him.)\n",
      "Estime Care. Wait old that happened anything.'It wasnt. Let more. Okay, she hands it's this?\n",
      "Joey: Reallyed good, and he walk, \"viso but in Sr. Dr. The hour doin'?  JOEY: What?\n",
      "Frank) Quoa, some to see Togo one colobanes the estract. Tagson.) Serious that?\n",
      "Chandler: Joey, one us... in enters. Im just too kiss that at him.\n",
      "Joey: \n",
      "Monica: That's elice, for him time, scome...cut be themerie, I mean I just.\" (Rachel returning off cours) Flive me a relaund!\n",
      "[Scene: TeVay.)\n",
      "Rachel: Hey.  ROSS: (on the helling rides and takich vize pall. \"Maymore. I wont a securited antnote, like-what mean theyre your hairp? But I'm new to tasker and walks at her and all bear. (Theyre awker puntible watching some another ringer.]\n",
      "Rachel: And how atheuted of selfight.]\n",
      "Ross: HuhN.\n",
      "Monica: (lauring) What, I did him to get her freaked it. \n",
      "Joey: Or made almosed found very in the lix butter one.)\n",
      "[Scene:usen Rachel's right, 308 la...'. to tall right.\n",
      "Rachel: That good boy at th me.\n",
      "Mike: Okay. Okay, maybe youy, I don't want to ready doo. You all the prettion. you know this haffece ways to guys.\n",
      "Ross: Joan. (Sits until to your funny.)\n",
      "Joey: Oh, I wasnt larged all in the sweats nicket for down at the shop.]  [Opens her is nice letting down.\n",
      "Chandler: (entering) Hey!!\n",
      "Dr. Sirce: Yay I think it it tofficicult!\n",
      "Closing Credits\n",
      "[Scene: Mr. Shes and we umbraketettle and me calliting up only put to me, while reachould at elliven the pain pretment.uffer to look and I'm going honelf. When it is it. \n",
      "(Swalks to leads her and beating honshe.]\n",
      "Mr. Nings again) Its Mo ya girls?  MONICA: Rach... man rae right? Y'know? You know I just sorrrad when I was, Day in That broom Dont know\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('Friends_Transcript.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e1006eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.4220, val loss 1.4670\n",
      "step 1000: train loss 1.4036, val loss 1.4410\n",
      "step 2000: train loss 1.3991, val loss 1.4416\n",
      "step 3000: train loss 1.3855, val loss 1.4317\n",
      "step 4000: train loss 1.3796, val loss 1.4279\n",
      "step 5000: train loss 1.3731, val loss 1.4274\n",
      "step 6000: train loss 1.3656, val loss 1.4363\n",
      "step 7000: train loss 1.3578, val loss 1.4079\n",
      "step 8000: train loss 1.3566, val loss 1.4032\n",
      "step 9000: train loss 1.3438, val loss 1.4036\n",
      "step 10000: train loss 1.3495, val loss 1.3977\n",
      "step 11000: train loss 1.3345, val loss 1.4138\n",
      "step 12000: train loss 1.3494, val loss 1.4013\n",
      "step 13000: train loss 1.3405, val loss 1.3995\n",
      "step 14000: train loss 1.3404, val loss 1.3964\n",
      "step 15000: train loss 1.3338, val loss 1.3981\n",
      "step 16000: train loss 1.3300, val loss 1.3845\n",
      "step 17000: train loss 1.3317, val loss 1.3926\n",
      "step 18000: train loss 1.3136, val loss 1.3752\n",
      "step 19000: train loss 1.3192, val loss 1.3908\n",
      "step 20000: train loss 1.3152, val loss 1.3781\n",
      "step 21000: train loss 1.3126, val loss 1.3699\n",
      "step 22000: train loss 1.3056, val loss 1.3773\n",
      "step 23000: train loss 1.3098, val loss 1.3804\n",
      "step 24000: train loss 1.3112, val loss 1.3621\n",
      "step 25000: train loss 1.3013, val loss 1.3715\n",
      "step 26000: train loss 1.3062, val loss 1.3696\n",
      "step 27000: train loss 1.2913, val loss 1.3717\n",
      "step 28000: train loss 1.3007, val loss 1.3688\n",
      "step 29000: train loss 1.3032, val loss 1.3660\n",
      "step 30000: train loss 1.3011, val loss 1.3689\n",
      "step 31000: train loss 1.2923, val loss 1.3593\n",
      "step 32000: train loss 1.2892, val loss 1.3532\n",
      "step 33000: train loss 1.2941, val loss 1.3548\n",
      "step 34000: train loss 1.2911, val loss 1.3674\n",
      "step 35000: train loss 1.2838, val loss 1.3605\n",
      "step 36000: train loss 1.2861, val loss 1.3459\n",
      "step 37000: train loss 1.2909, val loss 1.3303\n",
      "step 38000: train loss 1.2867, val loss 1.3429\n",
      "step 39000: train loss 1.2850, val loss 1.3342\n",
      "step 39999: train loss 1.2892, val loss 1.3519\n",
      "\n",
      "Rachel: Good used that movie\n",
      "[Scene: Monica and Rachel's placed that thats nippy. Make in Bonney Story rold.\n",
      "Ross: Its just not like from the best zoo, remastvonge. That needvest? Oh yeah! Yeah! Isnt because he would be helsexy (of Theighers.) Ohh my great!!\n",
      "Ross: What are you dad getting good presenty. Okay. But then I are fillowing out. Joey wer just and Mr. Geller: Not my fantairy and I was a rath, no-not wearing everything to tube to your ool people of Frain and Phoebe from goes back to the morning and realisive turning attempt.)\n",
      "Phoebe: Soap, honey, tell that merry now? Play-wil ya end it?\n",
      "Joey: So come on, I find on my asley. Im father a good.\n",
      "Rachel: Yeah... really hello? (The incle's feehiges peew like at Phoebs, Santa.  JOEY: Our What?  ROSS: Do you want the inch off cake, you and her coffee here, if I wouldn't see in the book head in they strang.  [Joey is stopple laundry guy.]\n",
      "Chandler: When you should even seven.\n",
      "Ross: Oh, well youre Doing?\n",
      "Frank: That is Phoebe Gustbian kids) Was a sheet worn that I-I dont can do?\n",
      "Joey: See uh Ross, I had to pregnant you! Did you know?! Pice doctors!\n",
      "Chandler: All right, dont do not make it, then is confidering a guy desk either.\n",
      "[Scene: The Fours negarbly and Rachels in the inle phone with Richard.\n",
      "Melan: I cant may come and or some ne, how Pheebs, richandy Rachel and, Rachel a jalar top the crotchen.]\n",
      "Joey: No-no, now I saided you doing any creject!\n",
      "Commercial Break\n",
      "[Scene: Millichs apartment teoses.]\n",
      "Ross: Handler! Look theres all Chandlers world, Monica in the door] Oh, is ojob, so [palents to Ross pauses is ones a long?\n",
      "Phoebe: Tomercially landows back following your shouldnt!\n",
      "Monica: It-Hi Mhattzers, Chandler, shes not today, IM thank you. [A machine song. Joey wacks nother made?\n",
      "Joey: Oh really pornsed over here?\n",
      "Ross: Ohh, you were so as a reason statiive my book guys. Geller groom on the best\n",
      "Mike: Ooooh, I better sound than Wadned-arable clail(les) All right, or, Paroloe rarnies to some bike to tell you! (He moves a guy does the strotch.   MONICA: Don't person in your Monica touched to start lying hands.\n",
      "Rachel: Thats what I had the direce on the sest other.)\n",
      "Rachel: One there funny leave a big tirest reason. I fat?\n",
      "Rachel: (holding her) The mrood best reason half is a boot kiss the nips riding something theres.\n",
      "ROSS: Oh.... and too! Joey anymore.\n",
      "Chandler: Miss, Chandler, lets do enters.]\n",
      "Checken: So what? Who can hear it was suren from Ross husimmed oes over his girlfriend, Monica makes him choldroom.) Whoa-whats it? (He promiliseling whattnartle tipe handiingnor ten boobshif Chopectause it hate?! Okay?  JOEY: Well, Eff course it's dranky. You said not a roof-utat?\n",
      "Joanna: Well if it's very button!\n",
      "Chandler: Umm. Jym and that's race-air-scenes! You have to cleaning the Pryace plushy!\n",
      "Chandler: Hew walks it\n",
      "Woman: I don't need what do what?\n",
      "Ross: ROss! Uh, so can make if then today okay?\n",
      "Monica: Okay, Ross. All-c.Set a thats verything and try things then, yeah, they chase your teason down informations a stand rooms wute turnible giframid?\n",
      "Chandler: We kinda notight. Hi finally dready.\n",
      "Monica: Phoebe we done great. Seriously especial Billion!\n",
      "Ross: Sitie you could were None DR. Terrible Francis molding homes finally controll.\n",
      "Monica: Oh. uh. (Monica gets doing about\n",
      "[Scene: Then all Ralph, Rachel, and I sent things sorry. I mean, affort. Heres in a movie. A\n",
      "Ross: Forgulo here, yknow?\n",
      "Ross: It worke that I'm loves Dinam. I'll say.  CHANDLER: Oh, ten, what was what that to disippy take that bathy's does anything for your and there's a strip back!\n",
      "Monica: Oh r. One the table of men. Um every today! , hugs outle but he's mold of the window may find wouldry.\n",
      "Ross: (on the droaping a battom.)\n",
      "Ross: He does the usen door went back the cheeks are coming into Alan and they sound, baja-zien minutes in her)\n",
      "Phoebe: Hi! (Sits down taking out two less taddate double.]\n",
      "FRSE: Like Yos for finding the picks or an accentbattom.\"  RACHEL: That does thats trying out tonight?\n",
      "Rachel Starks and get this one, gingers Monica are trying to asked the car has friendshillie what well I was makin\n",
      "Phoebe: What's that the party side man?\n",
      "Ross: Yeah, I guess I think that, I guess you hear them! Walk diner from her, in Danny Messaram from I need to really doctormiss on its stop winevolve our twenty.]\n",
      "CHANDLER: To. Be OK WIOR!  Fault Topo-conticial. \n",
      "Alley: You guys!\n",
      "Chandler: No I give the tickets tomorrow. Don't end one really goodbye?\n",
      "Phoebe: Yeah, thank, its so much I I-I take a nice!\n",
      "Chandler: (Monica huglan's from The TV and Rachel. They don't born one.  [She never goes to eno.]\n",
      "Joey: Sorry. (Patting the table.)\n",
      "[Scene: Monica and Monica's cunarlar. Ross orta kiss whis'.  CHANDLER: [Shake a hotic tap off the bed when Ive could make jackebratovasion tice tables, ok, I just geek a baloric thing, have a really love, and it does like which were a little full's looking. And thanche only the time openers agaiin all mend up.\n",
      "Hille: Okay there's a book.\n",
      "Ross: Come on! You move I guess healing some thants cheatning?\n",
      "Joey: (entering a knocking on the bers about the door enough, and Monica arent in overboy areas.\n",
      "Joey: Hey! (Cano, if I seem a table there. REgalh. I used, I-I lie, I take the parricl nies note with the only for from Silleing I very as uh all I got to cry) Hi palents, put a sorry proof in their high your though.}\n",
      "Rachel: Tells... and I was seeing what the matterifical loudly too. Joey does the boy stops right his juildare of huh, and and then unbattlio pincha!\n",
      "Rachel: You'd be chunch you.\n",
      "Joey: Swiun!\n",
      "Monica: Dr. God.See Lanastman I'm which, obset. The door-thing in the door and Alframes stealing.\n",
      "[Scene: Joey's apartment, Rachel is colling a farches-old is door.]\n",
      "Joey: What?\n",
      "Chandler: I dont think Joey? (Mindy Studends her and Monday Andam. I couldnt be okay.\n",
      "Monica: Ok, lets to the room niesmple serviewe mall a biking him batch.  JOEY: Oknow.  [Scene: Joey's Eamens openers to his expars and leans over gimme a touch coppens.\n",
      "Phoebe: You are.\n",
      "Ross: Sorry! Joey!\n",
      "Ross: Yeah?\n",
      "Monica: Umm, I'm looking out with you. \n",
      "(Monica looks are saking the chair)\n",
      "Ross: (Badsicaling with Joey) Oh no! This old and you have the wedding?\n",
      "Ross: Its a cute debull! We're bother chaying.\n",
      "Monica: (he rains theroats at a boton with Joey and mensele I got makin you?\n",
      "Ross: That's it comment!\n",
      "Rachel: Oh thank you. He isname?\n",
      "Familie: How are they bothere? (To the phone.)\n",
      "Rachel: Wow! Ummm, By than hard to woman hour a lot of your flirting the couch grans to him for a serious and me probably at her and schoulder]\n",
      "Ross: Hey Rachel!\n",
      "Ross: Hey!\n",
      "PheeESS: I GAO RONDFJ OHERISTES are time?  CHANDLER: [sophillicaK nights around.) Hey!\n",
      "Joey: Hey! \"She gates lames funnys even what you are still teachese for the deplaine. Would you do that?\n",
      "Phoebe: Yeah. Make is black Phatt and Rachel is\n",
      "Rachel: Hi. Listen, I know so Marcel that tell me ace whats wrong hairly filled why?\n",
      "Girls Salons: Oh yeah. Youre gotta believe nother herrtical cleaner)\n",
      "Monica: EurPropes!\n",
      "Ross: Listen, this   the twey invitable and talks and then a baskup its boob hes surensibulle. But you gotta guy nope of me you looking? He hands at us!!\n",
      "Joey: Hi! Thanks fire! (Dinnerolverly) Yknow what-whatt. It was that. (He butt is palk, and touch one of probleish Joannan is saiding the love in it.\n",
      "Ross: What-what thembractically wine bignile tajokes lapping them.)\n",
      "Mrs. Laman: Actual. (Reax]  ROSS: So. Why's it ticking me over your and you could knick me, there, Im not going on that stop out one mild-and New Wake!\n",
      "Rachel: Why?\n",
      "Carol: I'm a ael-aprocarily note.\n",
      "Ross: Jill night me in Regraioin back Pete: (To Monica apartment) O-oh. (they sycall framili, tell Monica's ler and then listens again.)\n",
      "[Scene: Rachel apartments Belies. Me enther needorlle) Oh my, I found a satuped out of clorrygive Eoking Central Perk, Phoebe creams Days Terrying's back in the book in a lentoy lipse, he leaves centy dressert) Hey! Right! Here.\n",
      "Joey: Whell I need join leaving?\n",
      "The Secreably AP Wandy Lips.\n",
      "Rachel: (To Monica) Joey, thank you guys see me qun.\n",
      "Chandler: Maybe turnatough so out.\n",
      "Chandler: (coming to hits her turp. Should decisi. You nebs Poke Trish her wedding you to he opened to Fund the crash far, sold you'll get the call... and could you get that and is still catchese on the job.] \n",
      "Ross: So, you're not youre sitting right I'm gonna put the bummastve Berttil minus?\n",
      "Chandler: Oh my God!\n",
      "Monica: Thank you!\n",
      "Monica: Oh, it parents is Last Betbone to him and Marcel leans for some corkin: You guys, very this, before my stuff.  PARIE: Or, he'll  make a canvolute back. And now a picture? Joey is saididicted left? I just not just any lickers father.\n",
      "Ross: Oh, really funnys, Pheebs, guys feels. And the door you want to terrible thing youre jall. Nope.\n",
      "Ross: OK, oh. Ok. Gosh Paserador's really strow her as sounder door) Ok, okay.\n",
      "Monica: Hey, Rach, its that all that youre not gonna sprinzing, you hope me, norra moy myself and I was just ugleping around that but.\n",
      "Phoebe: It's me, like really phoney that I can't?\n",
      "Rachel: Okay, Rachel, what does that? (Realises) Oh-oh. No, you give yearly knows?\n",
      "Rachel: I'm a tearian loob!\" But the tries and starts her inteels the mail) Hi!\n",
      "Ross: Look! No! I can too Really sychel's trouble. \n",
      "Joey: Euror wouldnt do it.\n",
      "Chandler: France.\n",
      "Joey: So, I have so left.\n",
      "Phoebe: I need the totally kidne. As youve hear heavy sputted another tonight)\n",
      "Phoebe: Do you can go!\n",
      "Chandler: What?, I thought you guys never happened, Monica starts to talk a conversation of quickly, and little connees. Ross pocket.\n",
      "Chandler: No.\n",
      "Monica: Okay! Did you mean.\n",
      "Coldrip: Uh-well, go whats it was totailly dool that all that for you guys! I wouldnt do that give you back them your fault, is too dinatrible?\n",
      "Phoebe: Oh, thats all my bowled toast year? Yeah, ok, how now don: Okay, and you sit nother drilling a list-right not that guy on the to salt.\n",
      "Joey: (you enters) All right?\n",
      "Phoebe: (Stens) Im gonna brought like her!\n",
      "Ross: Sorry that Joey! I don't could hear I'll be just there's a \n"
     ]
    }
   ],
   "source": [
    "max_iters = 40000\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77d2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
